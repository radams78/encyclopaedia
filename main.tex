\documentclass{book}
\title{Mathematics}
\author{Robin Adams}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\let\proof\relax
\let\endproof\relax
\let\qed\relax
\usepackage{pf2}
\usepackage{tikz-cd}

\newtheorem{ax}{Axiom}
\newtheorem{prop}{Proposition}[chapter]
\newtheorem{thm}[prop]{Theorem}
\newtheorem{lm}[prop]{Lemma}
\newtheorem{cor}{Corollary}[prop]
\theoremstyle{definition}
\newtheorem{df}[prop]{Definition}
\newtheorem{ex}[prop]{Example}

\newcommand{\Ab}{\ensuremath{\mathbf{Ab}}}
\newcommand{\Alg}[1]{\ensuremath{{#1}-\mathbf{Alg}}}
\newcommand{\Aut}[2]{\ensuremath{\mathrm{Aut}_{#1} \left( {#2} \right)}}
\newcommand{\bra}[1]{\ensuremath{\left\langle {#1} \right|}}
\newcommand{\coker}{\ensuremath{\operatorname{coker}}}
\newcommand{\End}[2]{\ensuremath{\mathrm{End}_{#1} \left( {#2} \right)}}
\newcommand{\Fab}[1]{\ensuremath{F^{\mathrm{ab}} \left( {#1} \right)}}
\newcommand{\gl}[2]{\ensuremath{\mathfrak{gl}_{#1} \left( {#2} \right)}}
\newcommand{\Grp}{\ensuremath{\mathbf{Grp}}}
\newcommand{\id}[1]{\ensuremath{\mathrm{id}_{#1}}}
\newcommand{\im}{\ensuremath{\operatorname{im}}}
\newcommand{\Inn}[1]{\ensuremath{\mathrm{Inn} \left( {#1} \right)}}
\newcommand{\inv}[1]{\ensuremath{{#1}^{-1}}}
\newcommand{\ket}[1]{\ensuremath{\left| {#1} \right\rangle}}
\newcommand{\lcm}{\ensuremath{\operatorname{lcm}}}
\renewcommand{\mod}{\ensuremath{\operatorname{mod}}}
\newcommand{\Mod}[1]{\ensuremath{{#1}-\mathbf{Mod}}}
\newcommand{\Ring}{\ensuremath{\mathbf{Ring}}}
\newcommand{\Set}{\ensuremath{\mathbf{Set}}}
\newcommand{\SL}[2]{\ensuremath{\mathrm{SL}_{#1} \left( {#2} \right)}}
\renewcommand{\sl}[2]{\ensuremath{\mathfrak{sl}_{#1} \left( {#2} \right)}}
\newcommand{\so}[2]{\ensuremath{\mathfrak{so}_{#1} \left( {#2} \right)}}
\newcommand{\Spec}{\ensuremath{\operatorname{Spec}}}
\newcommand{\Stab}[2]{\ensuremath{\mathrm{Stab}_{#1} \left( {#2} \right)}}
\newcommand{\tr}{\ensuremath{\operatorname{tr}}}
\newcommand{\Tr}{\ensuremath{\operatorname{Tr}}}
\newcommand{\Vect}[1]{\ensuremath{\mathbf{Vect}_{#1}}}

\begin{document}

\maketitle
\tableofcontents

% Definition
% Examples
% Basic Properties
% Relation with Previous Concepts
% Ways of Constructing X
% Special Kinds

\part{Category Theory}

\input{categories}

\part{Number Theory}

\begin{df}[Partition]
A \emph{partition} of a natural number $n$ is a nonincreasing sequence of positive integers whose sum is $n$.
\end{df}

\part{Order Theory}

\chapter{Boolean Algebras}

\begin{df}[Boolean Algebra]
A \emph{Boolean algebra} $B$ is a lattice %TODO Define this
with a function $\neg : B \rightarrow B$ such that, for all $a,b \in B$, we have
\begin{align*}
a \leq \neg b & \text{ iff } a \wedge b = \bot \\
\neg \neg a & = a
\end{align*}
\end{df}

\begin{ex}
For any set $A$, the power set $\mathcal{P} A$ is a Boolean algebra under inclusion, with $\neg X = A - X$.
\end{ex}

\begin{df}[Boolean Algebra Homomorphism]
Given Boolean algebras $B$ and $B'$, a \emph{Boolean algebra homomorphism} $h : B \rightarrow B'$ is a lattice homomorphism such that
\[ \forall x \in B. h(\neg x) = \neg h(x) \enspace . \]
Let $\mathbf{BA}$ be the category of Boolean algebras and Boolean algebra homomorphisms.
\end{df}

\begin{ex}
$\mathbf{2} = \mathcal{P} \mathbf{1}$ is initial in $\mathbf{BA}$.
\end{ex}

\begin{df}[Filter]
Let $B$ be a Boolean algebra. A \emph{filter} in $B$ is a subset $F \subseteq B$ that is closed upwards and closed under binary meets.
\end{df}

\begin{df}[Maximal Filter]
A filter $F$ in $B$ is \emph{maximal} or an \emph{ultrafilter} iff $F \neq B$ and, for any filter $F'$, if $F \subseteq F'$ then either $F = F'$ or $F' = B$.
\end{df}

\begin{prop}
Let $F$ be a filter in $B$. Then $F$ is an ultrafilter iff, for all $x \in B$, exactly one of $x \in F$ and $\neg x \in F$ holds.
\end{prop}

\begin{proof}
\pf
\step{1}{If $F$ is an ultrafilter then, for all $x \in B$, we have either $x \in F$ or $\neg x \in F$.}
\begin{proof}
	\step{a}{\assume{$F$ is an ultrafilter.}}
	\step{b}{\pflet{$x \in B$}}
	\step{c}{\pflet{$F' = \{y \in B \mid \neg x \vee y \in F \}$}}
	\step{d}{$F \subseteq F'$}
	\step{e}{$F' = F$ or $F' = B$}
	\step{f}{\case{$F' = F$}}
	\begin{proof}
		\pf\ We have $x \in F'$ hence $x \in B$.
	\end{proof}
	\step{g}{\case{$F' = B$}}
	\begin{proof}
		\pf\ We have $\bot \in F'$ hence $\neg x \vee \bot \in F$ and so $\neg x \in F$.
	\end{proof}
\end{proof}
\step{2}{If $F$ is an ultrafilter then we do not have $x \in F$ and $\neg x \in F$.}
\begin{proof}
	\pf\ If $x \in F$ and $\neg x \in F$ then $\bot \in F$ hence $F = B$.
\end{proof}
\step{3}{If, for all $x \in B$, we have exactly one of $x \in F$ and $\neg x \in F$ holds, then $F$ is an ultrafilter.}
\begin{proof}
	\step{a}{\assume{For all $x \in B$, we have exactly one of $x \in F$ and $\neg x \in F$ holds.}}
	\step{b}{\pflet{$F'$ be a filter with $F \subset F'$.} \prove{$F' = B$}}
	\step{c}{\pick\ $x \in F' - F$}
	\step{d}{$\neg x \in F$}
	\step{e}{$x, \neg x \in F'$}
	\step{f}{$F' = B$}
\end{proof}
\qed
\end{proof}

\part{Group Theory}

\input{groups}

\part{Ring Theory}

\input{rings}

\part{Linear Algebra}

\chapter{Vector Spaces}

\begin{df}[Vector Space]
Let $K$ be a field. A \emph{vector space} over $K$ is a module over $K$. A \emph{linear transformation} is a $K$-module homomorphism.
\end{df}

\begin{df}[Bilinear Map]
Let $K$ be a field. Let $U$, $V$ and $W$ be vector spaces over $K$. A function $f : U \times V \rightarrow W$ is \emph{bilinear} iff, for all $u_1, u_2 \in U$ and $v_1,v_2 \in V$ and $\alpha \in K$,
\begin{align*}
f(u_1 + \alpha u_2, v_1) & = f(u_1,v_1) + \alpha f(u_2,v_1) \\
f(u_1, v_1 + \alpha v_2) & = f(u_1,v_1) + \alpha f(u_1,v_2)
\end{align*}
\end{df}

\begin{thm}
Let $K$ be a field. Let $U$ and $V$ be vector spaces. There exists a vector space $U \otimes V$ over $K$ and bilinear map $- \otimes - : U \times V \rightarrow U \otimes V$, unique up to isomorphism, such that, for every vector space $W$ over $K$ and bilinear map $f : U \times V \rightarrow W$, there exists a unique linear map $\overline{f} : U \otimes V \rightarrow W$ such that the following diagram commutes.
\[ \begin{tikzcd}
U \otimes V \arrow[r,"\overline{f}"] & W \\
U \times V \arrow[u,"- \otimes -"] \arrow[ur,"f"]
\end{tikzcd} \]
Further, $- \otimes -$ is injective and its image spans $U \otimes V$.
\end{thm}

\begin{proof}
\pf\ We can construct $U \otimes V$ as follows. Let $L$ be the free vector space generated by $U \times V$. Let $R$ be the subspace generated by all vectors of the form
\begin{equation*}
(u_1 + \alpha u_2, v) - (u_1,v) - \alpha (u_2,v) \\
(u, v_1 + \alpha v_2) - (u,v_1) - \alpha (u,v_2)
\end{equation*}
Take $U \otimes V := L / R$. \qed
\end{proof}

\begin{prop}
\label{prop:linearly-disjoint}
If $\sum_{i=1}^n u_i \otimes v_i = 0$ and $v_1$, \ldots, $v_n$ are linearly independent in $V$ then $u_1 = \cdots = u_n = 0$.
\end{prop}

\begin{proof}
\pf
\step{1}{\pflet{$f : U \times V \rightarrow V^{U^*}$ be the function $f(u,v)(\Phi) = \Phi(u)v$}}
\step{2}{$f$ is bilinear.}
\step{3}{\pflet{$\overline{f} : U \otimes V \rightarrow V^{U^*}$ be the induced linear transformation.}}
\step{4}{$\overline{f}(\sum_{i=1}^n u_i \otimes v_i) = 0$}
\step{5}{$\sum_{i=1}^n f(u_i,v_i) = 0$}
\step{6}{For all $\Phi \in U^*$ we have $\sum_{i=1}^n \Phi(u_i) v_i = 0$}
\step{7}{For all $\Phi \in U^*$ we have $\Phi(u_1) = \cdots = \Phi(u_n) = 0$}
\step{8}{$u_1 = \cdots = u_n = 0$}
\qed
\end{proof}

\begin{prop}
Let $U$ and $V$ be vector spaces over $K$ with bases $\mathcal{B}_1$ and $\mathcal{B}_2$. Then $\mathcal{B} = \{ b_1 \otimes b_2 : b_1 \in \mathcal{B}_1, b_2 \in \mathcal{B}_2 \}$ is a basis for $U \otimes V$.
\end{prop}

\begin{proof}
\pf
\step{1}{$\mathcal{B}$ is linearly independent.}
\begin{proof}
	\step{a}{\assume{$\sum_{i=1}^m \sum_{j=1}^n \alpha_{ij} b_i \otimes b_j' = 0$}}
	\step{b}{For all $j$ we have $\sum_{i=1}^m \alpha_{ij} b_i = 0$}
	\begin{proof}
		\pf\ Proposition \ref{prop:linearly-disjoint}.
	\end{proof}
	\step{c}{Each $\alpha_{ij}$ is 0.}
\end{proof}
\step{2}{$\mathcal{B}$ spans $U \otimes V$.}
\begin{proof}
	\pf\ If $u = \alpha_1 b_1 + \cdots + \alpha_m b_m$ and $v = \beta_1 b_1' + \cdots + \beta_n b_n'$ then
	\[ u \otimes v = \sum_{i=1}^m \sum_{j=1}^n \alpha_i \beta_j (b_i \otimes b_j') \]
	The result follows since the vectors of the form $u \otimes v$ span $U \otimes V$.
\end{proof}
\qed
\end{proof}

\begin{cor}
If $U$ and $V$ are finite dimensional vector spaces over $K$ then
\[ \dim (U \otimes V) = (\dim U)(\dim V) \enspace . \]
\end{cor}

\begin{prop}
$\mathbf{Vect}_K$ is a symmetric monoidal category under $\otimes$.
\end{prop}

\section{Dual Spaces}

\begin{df}
Given vector spaces $U$ and $V$ over $K$, we make $\mathbf{Vect}_K[U,V]$ into a $K$-vector space by defining:
\begin{align*}
(S + T)(u) & = S(u) + T(u) \\
(\alpha T)(u) & = \alpha T(u)
\end{align*}
\end{df}

\begin{df}[Dual Space]
Given a vector space $V$ over $K$, the \emph{dual space} $V^*$ is
\[ V^* = \Vect{K}[V,K] \enspace . \]
An element of $V^*$ is called a \emph{linear functional} on $V$.
\end{df}

\begin{prop}
The natural transformation
\[ \eta_V : V \rightarrow V^{**} \]
defined by
\[ \eta_V(v)(f) = f(v) \enspace . \]
is a natural isomorphism if $V$ is finite dimensional.
\end{prop}

\begin{proof}
\pf
\step{1}{For all $v \in V$ we have $\eta_V(v) \in V^{**}$.}
\begin{proof}
	\pf
	\begin{align*}
		\eta_V(v)(f + \lambda g) & = (f + \lambda g)(v) \\
		& = f(v) + \lambda g(v) \\
		& = \eta_V(v)(f) + \lambda \eta_V(v)(g)
	\end{align*}
\end{proof}
\step{2}{$\eta_V$ is linear.}
\begin{proof}
	\pf
	\begin{align*}
		\eta_V(u + \lambda v)(f) & = f(u + \lambda v) \\
		& = f(u) + \lambda f(v) \\
		& = \eta_V(u)(f) + \lambda \eta_V(v)(f)
	\end{align*}
\end{proof}
\step{3}{$\eta_V$ is natural in $V$.}
\begin{proof}
	\step{i}{\pflet{$T : U \rightarrow V$ be a linear transformation.}}
	\step{ii}{$\eta_V \circ T = \Vect{K}[\Vect{K}[T, \id{K}], \id{K}] \circ \eta_U$}
	\begin{proof}
		\pf
		\begin{align*}
			\eta_V(T(u))(f) & = f(T(u)) \\
			(\Vect{K}[\Vect{K}[T,\id{K}],\id{K}](\eta_U(u)))(f) & = \eta_U(u)(\Vect{K}[T,\id{K}](f)) \\
			& = \eta_U(u)(f \circ T) \\
			& = f(T(u))
		\end{align*}
	\end{proof}
\end{proof}
\step{4}{$\ker \eta_V = \{0\}$}
\begin{proof}
	\pf
	\begin{align*}
		v \in \ker \eta_V & \Leftrightarrow \forall f \in V^*. f(v) = 0 \\
		& \Leftrightarrow v = 0
	\end{align*}
	using the fact that $V$ is finite dimensional.
\end{proof}
\step{5}{$\im \eta_V = V^{**}$}
\begin{proof}
	\step{a}{\pick\ a basis $\{ \ket{e_1}, \ldots, \ket{e_n} \}$ for $V$.}
	\step{b}{\pflet{$f \in V^{**}$}}
	\step{c}{For $i = 1, \ldots, n$, \pflet{$\alpha_i = f(\bra{e_i})$}}
	\step{d}{\pflet{$v = \sum_i \alpha_i \ket{e_i}$}}
	\step{e}{$f = \eta_V(v)$}
	\begin{proof}
		\pf
		\begin{align*}
			f(\bra{e_i}) & = \alpha_i \\
			& = \bra{e_i}(v) \\
			& = \eta_V(v)(\bra{e_i}) 
		\end{align*}
	\end{proof}
\end{proof}
\qed
\end{proof}

\section{Eigenvalues and Eigenvectors}

\begin{df}[Eigenvalue, Eigenvector]
Let $V$ be a vector space over $K$ and $T : V \rightarrow V$ a linear transformation. Then $v \in V$ is an \emph{eigenvector} of $T$ with \emph{eigenvalue} $\lambda$ iff
\[ T(v) = \lambda v \enspace . \]
\end{df}

\begin{prop}
For $\lambda \in K$, the set of all eigenvectors with eigenvalue $\lambda$ forms a subspace of $V$.
\end{prop}

\begin{proof}
\pf\ If $u$ and $v$ are $\lambda$-eigenvectors then
\begin{align*}
T(u + \alpha v) & = T(u) + \alpha T(v) \\
& = \lambda u + \alpha \lambda v \\
& = \lambda (u + \alpha v) & \qed
\end{align*}
\end{proof}

\begin{df}[Eigenspace]
For $\lambda \in K$, the \emph{eigenspace} of $\lambda$ is the subspace of all eigenvectors with eigenvalue $\lambda$.
\end{df}

\begin{df}[Degenerate]
We say $\lambda \in K$ is a \emph{degenerate} eigenvalue iff its eigenspace has dimension $> 1$, and \emph{non-degenerate} iff its eigenspace has dimension 1.
\end{df}

\section{Commutators and Anticommutators}

\begin{df}[Commutator]
The \emph{commutator} of linear transformations $S,T : V \rightarrow V$ is $[S,T] = ST - TS$.
\end{df}

\begin{df}[Anticommutator]
The \emph{antycommutator} of linear transformations $S,T : V \rightarrow V$ is $\{ S,T \} = ST + TS$.
\end{df}

\chapter{Inner Product Spaces}

\begin{df}[Orthogonal]
Vectors $u$ and $v$ are \emph{orthogonal} iff $\langle u \mid v \rangle = 0$.
\end{df}

\begin{df}[Norm]
The \emph{norm} of a vector $v$ is defined by
\[ \| v \| = \langle v \mid v \rangle \enspace . \]
\end{df}

\begin{prop}
Let $V$ and $W$ be complex inner product spaces and $T : V \rightarrow W$ linear. If $V$ is finite dimensional then $T$ is bounded.
\end{prop}

\begin{proof}
\pf
\step{1}{\pick\ an orthonormal basis $\ket{v_1}$, \ldots, $\ket{v_n}$.}
\step{2}{\pflet{$B = \max(\| T\ket{v_1} \|, \ldots, \| T \ket{v_n} \|)$}}
\step{3}{For all $v \in V$ we have $\| Tv \| \leq B \|v\|$}
\begin{proof}
	\pf
	\begin{align*}
		\| T (\alpha_1 v_1 + \cdots + \alpha_n v_n) \|
		& = \| \alpha_1 T v_1 + \cdots + \alpha_n T v_n \| \\
		& \leq |\alpha_1| \| T v_1 \| + \cdots + |\alpha_n| \|T v_n \| \\
		& \leq B (|\alpha_1| + \cdots + |\alpha_n|) \\
		& = B \| \alpha_1 v_1 + \cdots + \alpha_n v_n \|
	\end{align*}
\end{proof}
\qed
\end{proof}

\begin{df}[Bra]
Given a vector $\ket{v} \in V$, define the \emph{bra} $\bra{v} \in V^*$ by:
\[ (\bra{v})(\ket{u}) = \langle v | u \rangle \enspace . \]
\end{df}

\begin{prop}
\begin{align*}
\bra{u + v} & = \bra{u} + \bra{v} \\
\bra{\alpha v} & = \overline{\alpha} \bra{v}
\end{align*}
\end{prop}

\begin{prop}[Schwarz Inequality]
Let $V$ be an inner product space and $\ket{\alpha}, \ket{\beta} \in V$. Then
\[ \| \alpha\| ^2 \| \beta \|^2 \geq |\langle \alpha \mid \beta \rangle|^2 \enspace . \]
\end{prop}

\begin{proof}
\pf
\step{1}{For all $\lambda \in \mathbb{C}$ we have
\[ (\bra{\alpha} + \overline{\lambda} \bra{\beta}) (\ket{\alpha} + \lambda \ket{\beta}) \geq 0 \enspace . \]}
\step{2}{$\langle \alpha \mid \alpha \rangle \langle \beta \mid \beta \rangle - |\langle \alpha \mid \beta \rangle|^2 \geq 0$}
\begin{proof}
	\pf\ Taking $\lambda = - \langle \beta \mid \alpha \rangle \langle \beta \mid \beta \rangle$ in \stepref{1}.
\end{proof}
\qed
\end{proof}

\chapter{Hilbert Spaces}

\begin{df}[Hilbert Space]
A \emph{Hilbert space} is a complete complex inner product space.
\end{df}


\begin{df}[Separable]
A Hilbert space is \emph{separable} iff there exists a countable orthonormal basis that is dense.
\end{df}

\begin{df}[Bra]
Let $\mathcal{H}$ be a Hilbert space and $\ket{\psi} \in \mathcal{H}$. We define the \emph{bra} $\bra{\psi} : \mathcal{H} \rightarrow \mathbb{C}$ to be the linear functional
\[ \bra{\psi} (\ket{\phi}) = \langle \psi | \phi \rangle \enspace . \]
\end{df}

\begin{prop}
\[ \bra{\psi + \phi} = \bra{\psi} + \bra{\phi} \]
\end{prop}

\begin{proof}
\pf\ Since $\langle \psi + \phi \mid \chi \rangle = \langle \psi \mid \chi \rangle + \langle \phi \mid \chi \rangle$. \qed
\end{proof}

\begin{prop}
\[ \bra{\alpha \psi} = \overline{\alpha} \bra{\psi} \]
\end{prop}

\begin{proof}
\pf\ Since $\langle \alpha \psi \mid \chi \rangle = \overline{\alpha} \langle \psi \mid \chi \rangle$. \qed
\end{proof}

\begin{df}[Outer Product]
Let $\mathcal{H}$ be a Hilbert space and $\ket{\phi}, \ket{\psi} \in \mathcal{H}$. Define the \emph{outer product}
\[ \ket{\psi} \bra{\phi} : \mathcal{H} \rightarrow \mathcal{H} \]
to be the linear transformation
\[ (\ket{\psi} \bra{\phi})(\ket{\chi}) = \langle \phi \mid \chi \rangle \ket{\psi} \enspace . \]
\end{df}

\begin{prop}[Completeness Relation]
If $\{ \ket{e_1}, \ldots, \ket{e_n} \}$ is an orthonormal basis for $\mathcal{H}$, then
\[ \sum_{i=1}^n \ket{e_i} \bra{e_i} = \id{\mathcal{H}} \enspace . \]
\end{prop}

\begin{proof}
\pf
\begin{align*}
\left( \sum_{i=1}^n \ket{e_i} \bra{e_i} \right) \sum_{j=1}^n \alpha_j \ket{e_j}
& = \sum_{i=1}^n \sum_{j=1}^n \alpha_j \langle e_i \mid e_j \rangle \ket{e_j} \\
& = \sum_{j=1}^n \alpha_j \ket{e_j} & \qed
\end{align*}
\end{proof}

\begin{prop}
Given a bounded linear operator $T : \mathcal{H} \rightarrow \mathcal{H}$, there exists a unique linear operator $T^\dagger : \mathcal{H} \rightarrow \mathcal{H}$ such that, for all $\ket{\phi}, \ket{\psi} \in \mathcal{H}$,
\[ \langle T \psi \mid \phi \rangle = \langle \psi \mid T^\dagger \mid \phi \rangle \]
\end{prop}

%TODO

\begin{df}[Adjoint]
Given a bounded linear operator $T : \mathcal{H} \rightarrow \mathcal{H}$, the \emph{adjoint} of $T$ is the linear operator $T^\dagger : \mathcal{H} \rightarrow \mathcal{H}$ such that, for all $\ket{\phi}, \ket{\psi} \in \mathcal{H}$,
\[ \langle T \psi \mid \phi \rangle = \langle \psi \mid T^\dagger \mid \phi \rangle \]
\end{df}

\begin{prop}
\[ (S \circ T)^\dagger = T^\dagger \circ S^\dagger \]
\end{prop}

\begin{proof}
\pf
\begin{align*}
\langle \psi \mid (S \circ T)^\dagger \mid \phi \rangle
& = \langle S(T\psi) \mid \phi \rangle \\
& = \langle T \psi \mid S^\dagger \phi \rangle \\
& = \langle T \psi \mid T^\dagger (S^\dagger \phi) \rangle & \qed
\end{align*}
\end{proof}

\begin{prop}
\label{prop:dirac-triple}
\[ \overline{\langle \alpha \mid T \mid \beta \rangle} = \langle \beta \mid T^\dagger \mid \alpha \rangle \]
\end{prop}

\begin{proof}
\pf\ Immediate from definitions. \qed
\end{proof}

\begin{df}[Hermitian]
A linear operator $T : \mathcal{H} \rightarrow \mathcal{H}$ is \emph{Hermitian} or \emph{self-adjoint} iff $T$ is bounded and $T^\dagger = T$.
\end{df}

\begin{prop}
If $T$ is Hermitian then
\[ \overline{\langle \alpha \mid T \mid \beta} = \langle \beta \mid T \mid \alpha \rangle \]
\end{prop}

\begin{proof}
\pf\ Proposition \ref{prop:dirac-triple}. \qed
\end{proof}

\begin{thm}
The eigenvalues of a Hermitian operator are real.
\end{thm}

\begin{proof}
\pf
\step{1}{\pflet{$T$ be Hermitian.}}
\step{2}{\pflet{$T \ket{v} = \alpha \ket{v}$}}
\step{3}{\assume{w.l.o.g. $\| \ket{v} \| = 1$}}
\step{4}{$\langle v \mid T \mid v \rangle = \alpha$}
\step{5}{$\alpha = \overline{\alpha}$}
\begin{proof}
	\pf
	\begin{align*}
		\overline{\alpha} & = \overline{\langle v \mid T \mid v \rangle} \\
		& = \langle v \mid T^\dagger \mid v \rangle \\
		& = \langle v \mid T \mid v \rangle \\
		& = \alpha
	\end{align*}
\end{proof}
\qed
\end{proof}

\begin{thm}
Let $T$ be a Hermitian operator. Then eigenvectors of $T$ with different eigenvalues are orthogonal.
\end{thm}

\begin{proof}
\pf
\step{1}{\pflet{$T \ket{u} = \alpha \ket{u}$ and $T \ket{v} = \beta \ket{v}$ where $\alpha \neq \beta$.}}
\step{2}{$\alpha \langle u \mid v \rangle = \beta \langle u \mid v \rangle$}
\begin{proof}
	\pf
	\begin{align*}
		\alpha \langle u \mid v \rangle
		& = \langle u \mid T \mid v \rangle \\
		& = \overline{\langle v \mid T \mid u \rangle} \\
		& = \beta \overline{\langle v \mid u \rangle} & (\beta \text{ is real}) \\
		& = \beta \langle u \mid v \rangle
	\end{align*}
\end{proof}
\step{3}{$\langle u \mid v \rangle = 0$}
\qed
\end{proof}

\begin{df}[Projection]
A linear operator $P : \mathcal{H} \rightarrow \mathcal{H}$ is a \emph{projection} iff $P = P^2 = P^\dagger$.
\end{df}

\begin{df}[Unitary]
A linear operator $U : \mathcal{H} \rightarrow \mathcal{H}$ is \emph{unitary} iff $U^\dagger U = \id{\mathcal{H}}$.
\end{df}

\begin{df}[Trace]
Let $\{ \ket{e_1}, \ldots, \ket{e_n} \}$ be an orthonormal basis for $\mathcal{H}$. The \emph{trace} of a linear operator $T : \mathcal{H} \rightarrow \mathcal{H}$ is
\[ \Tr(T) = \sum_{i=1}^n \langle e_i \mid T \mid e_i \rangle \enspace . \]
\end{df}

%TODO Prove this is independent of the choice of basis

\begin{df}[Positive Semidefinite]
A Hermitian operator $T : \mathcal{H} \rightarrow \mathcal{H}$ is \emph{positive semidefinite} iff, for all $\psi \in \mathcal{H}$, we have
\[ \langle \psi \mid T \mid \psi \rangle \geq 0 \enspace . \]
\end{df}

\begin{df}[Outer Product]
Given vectors $\ket{\alpha}, \ket{\beta} \in H$, the \emph{outer product} $\ket{\beta} \bra{\alpha} : H \rightarrow H$ is the linear transformation defined by
\[ (\ket{\beta} \bra{\alpha})(\ket{\gamma}) = \langle \alpha \mid \gamma \rangle \ket{\beta} \enspace . \]
\end{df}

\begin{prop}
\[ (\ket{\beta} \bra{\alpha})^\dagger = \ket{\alpha} \bra{\beta} \]
\end{prop}

\begin{proof}
\pf
\begin{align*}
\langle \psi \mid \alpha \rangle \langle \beta \mid \phi \rangle
& = \langle \langle \alpha \mid \psi \rangle \beta \mid \phi \rangle \\
& = \langle (\ket{\beta} \bra{\alpha})(\ket{\psi}) \mid \phi \rangle & \qed
\end{align*}
\end{proof}

\begin{df}[Expectation Value]
Let $A : H \rightarrow H$ be a linear transformation and $\ket{\alpha} \in H$ be normalized. The \emph{expectation} value of $A$ with respect to $\ket{\alpha}$ is
\[ \langle A \rangle_{\ket{\alpha}} = \langle \alpha \mid A \mid \alpha \rangle \enspace . \]
\end{df}

\begin{prop}
The expectation value of a Hermitian operator is real.
\end{prop}

\begin{proof}
\pf\ Since $\langle \alpha \mid A \mid \alpha \rangle = \overline{\langle \alpha \mid A \mid \alpha \rangle}$. \qed
\end{proof}

\begin{prop}
The expectation value of an anti-Hermitian operator is purely imaginary.
\end{prop}

\begin{proof}
\pf\ If $A$ is anti-Hermitian then
\begin{align*}
\overline{\langle \alpha \mid A \mid \alpha \rangle}
& = \langle \alpha \mid A^\dagger \mid \alpha \rangle \\
& = - \langle \alpha \mid A \mid \alpha \rangle & \qed
\end{align*}
\end{proof}

\begin{df}[Dispersion]
Let $A$ be a Hermitian operator on $H$ and $\ket{\alpha} \in H$ be normalized. Define
\[ \Delta A = A - \langle A \rangle_{\ket{\alpha}} \enspace . \]
The \emph{dispersion}, \emph{variance} or \emph{mean square deviation} of $A$ is $\langle (\Delta A)^2 \rangle_{\ket{\alpha}}$.
\end{df}

\begin{prop}
\[ \langle (\Delta A)^2 \rangle_{\ket{\alpha}} = \langle A^2 \rangle_{\ket{\alpha}} - \langle A \rangle_{\ket{\alpha}}^2 \]
\end{prop}

\begin{proof}
\pf
\begin{align*}
\langle (\Delta A)^2 \rangle & = \langle A^2 - 2 \langle A \rangle A + \langle A \rangle^2 \rangle \\
& = \langle A^2 \rangle - 2 \langle A \rangle^2 + \langle A \rangle^2 \\
& = \langle A^2 \rangle - \langle A \rangle^2 & \qed
\end{align*}
\end{proof}
\chapter{Lie Algebras}

\begin{df}[Lie Algebra]
Let $K$ be a field. A \emph{Lie algebra} $\mathcal{L}$ over $K$ is a vector space over $K$ with an operation
\[ [\ ,\ ] : \mathcal{L}^2 \rightarrow \mathcal{L} \enspace , \]
the \emph{Lie bracket} or \emph{commutator}, such that, for all $\alpha \in K$ and $x,y,z \in \mathcal{L}$:
\begin{align*}
[x+y,z] & = [x,z] + [y,z] \\
[x,y+z] & = [x,y] + [x,z] \\
[\alpha x,y] & = \alpha [x,y] \\
[x,x] & = 0 \\
[x,[y,z]] + [y,[z,x]] + [z,[x,y]] & = 0
\end{align*}
The last equation is called the \emph{Jacobi identity}.
\end{df}

\begin{prop}
If $K$ is either $\mathbb{R}$ or $\mathbb{C}$, then the condition $[x,x] = 0$ is equivalent to the \emph{skew-symmetry condition}:
\[ [x,y] = -[y,x] \enspace . \]
\end{prop}

%TODO

\begin{ex}
$\mathbb{R}^3$ is a Lie algebra under the cross product.
\end{ex}

\begin{ex}
$\gl
{n}{\mathbb{R}}$ and $\gl{n}{\mathbb{C}}$ are Lie algebras under
\[ [A,B] = AB - BA \enspace . \]
A sub-Lie algebra of one of these is called a \emph{linear} Lie algebra.
\end{ex}

\begin{ex}
$\sl{n}{\mathbb{R}}$ is a linear Lie algebra.
\end{ex}

\begin{ex}
$\so{n}{\mathbb{R}}$ is a linear Lie algebra.
\end{ex}

\begin{ex}
The set $u(n)$ of skew-Hermitian $n \times n$ matrices is a real linear Lie algebra.
\end{ex}

\begin{ex}
$su(n) = \{ M \in u(n) : \tr U = 0 \}$ is a sub-Lie algebra of $u(n)$.
\end{ex}

\begin{df}[Pauli matrices]
The \emph{Pauli matrices} are
\begin{align*}
\sigma_x & = \left( \begin{array}{cc}
0 & 1 \\ 1 & 0
\end{array} \right) \\
\sigma_y & = \left( \begin{array}{cc}
0 & -i \\ i & 0
\end{array} \right) \\
\sigma_z & = \left( \begin{array}{cc}
1 & 0 \\ 0 & -1
\end{array} \right)
\end{align*}
\end{df}

\begin{prop}
The Pauli matrices span $su(2)$.
\end{prop}

\begin{prop}
\begin{align*}
[\sigma_x, \sigma_y] & = \sigma_z \\
[\sigma_y, \sigma_z] & = \sigma_x \\
[\sigma_z, \sigma_x] & = \sigma_y
\end{align*}
\end{prop}

\begin{cor}
Any two of the Pauli matrices generate $su(2)$ as a Lie algebra.
\end{cor}

\section{Lie Algebra Homomorphisms}

\begin{df}[Lie Algebra Homomorphism]
A \emph{Lie algebra homomorphism} is a linear transformation that preserves the Lie bracket.

Let $\mathbf{Lie}_K$ be the category of Lie algebras over $K$.
\end{df}

\begin{prop}
The forgetful functor $\mathbf{Lie}_K \rightarrow \Set$ reflects isomorphisms; i.e. bijective Lie homomorphisms are isomorphisms.
\end{prop}

%TODO

\begin{ex}
$su(2)$ is isomorphic to $\mathbb{R}^3$ under the cross product, with the isomorphism given by
\[ \sigma_x \mapsto \vec{i}, \sigma_y \mapsto \vec{j}, \sigma_z \mapsto \vec{k} \]
\end{ex}

\chapter{Lie Groups}

\begin{df}[Lie Group]
A \emph{Lie group} is a group object in the category of analytic differentiable manifolds.
\end{df}

\begin{ex}
$GL(n,\mathbb{C})$ is a Lie group.
\end{ex}

\begin{ex}
$U(n)$ is a Lie subgroup of $GL(n,\mathbb{C})$.
\end{ex}

\begin{ex}
$SU(n)$ is a Lie subgroup of $U(n)$.
\end{ex}

\begin{prop}
The forgetful functor $\mathbf{LieGroup} \rightarrow \Set$ reflects isomorphisms.
\end{prop}

%TODO

\begin{df}[Tangent Vector at the Identity]
Let $G$ be a Lie group. Let $N$ be a neighbourhood of $e$. The set of \emph{tangent vectors at the identity} at $N$ is the set of curves $\gamma : \mathbb{R} \rightarrow N$ such that $\gamma(0) = e$ quotiented by: $\gamma_1 = \gamma_2$ iff, for every smooth function $f : N \rightarrow \mathbb{R}$,
\[ (f \circ \gamma_1)'(0) = (f \circ \gamma_2)'(0) \enspace . \]
\end{df}

\begin{prop}
The function that maps $\gamma$ to
\[ \lambda f. (f \circ \gamma)'(0) \]
 is a bijection between the tangent vectors at the identity and the set of derivations on the space of the smooth functions $N \rightarrow \mathbb{R}$.
 \end{prop}
 
 %TODO
 
\begin{df}[Tangent Space]
The \emph{tangent space} is the set of tangent vectors at the identity under the vector space structure inherited from the space of derivations on $C^\infty[N, \mathbb{R}]$.
\end{df}

\begin{prop}
The dimension of the tangent space is the same as the dimension of $G$.
\end{prop}

%TODO

\part{Topology}

\begin{prop}
Let $\{ X_i \}_{i \in I}$ be a family of topological spaces. Then there is a product $\prod_i X_i$ in $\mathbf{Top}$ which consists of the set $\prod_i X_i$ under the topology generated by $\inv{\pi_i}(U)$ for all $i \in I$ and $U$ open in $X_i$.
\end{prop}

\part{Measure Theory}

\begin{df}[$\sigma$-algebra]
Let $X$ be a set. A \emph{$\sigma$-algebra} on $X$ is a nonempty set $\Sigma \subseteq \mathcal{P} X$ that is closed under complement, countable union, and countable intersection.

A \emph{measurable space} consists of a set with a $\sigma$-algebra.
\end{df}

\begin{df}[Measure]
Let $(X, \sigma)$ be a measurable space. A \emph{measure} on $(X, \sigma)$ is a function $\mu : \Sigma \rightarrow \mathbb{R}_{\geq 0} \cup \{ + \infty \}$ such that:
\begin{itemize}
\item $\mu(\emptyset) = 0$
\item For any countable set of pairwise disjoint sets $\{ E_n : n \in \mathbb{N} \}$ in $\Sigma$,
\[ \mu \left( \bigcup_{n=0}^\infty E_n \right) = \sum_{n=0}^\infty \mu(E_n) \enspace . \]
\end{itemize}
\end{df}

\part{Quantum Theory}

Associated with any physical system $S$ is a Hilbert space $H_S$ such that the states of $S$ correspond to the non-zero vectors in $H_S$ quotiented by: $\ket{\alpha}$ and $c \ket{\alpha}$ represent the same physical state ($c \neq 0$).

Associated with any \emph{observable} property $P$ of the system $S$ is a Hermitian operator $T_P : H_S \rightarrow H_S$.

If we measure the property $P$, then the value measured is an eigevalue of $T_P$, and after the measurement the state is an eigenvector of $T_P$ with that eigenvalue.

\begin{ex}
A \emph{spin-$1/2$ system} has a 2-dimensional Hilbert space with basis $\{ \ket{+}, \ket{-} \} = \{ \left( \begin{array}{c} 1 \\ 0 \end{array} \right), \left( \begin{array}{c} 0 \\ 1 \end{array} \right) \}$.

We have the observables
\begin{align*}
S_x & = \frac{\hbar}{2} (\ket{+} \bra{-} + \ket{-} \bra{+}) \\
& = \frac{\hbar}{2} \left( \begin{array}{cc}
0 & 1 \\ 1 & 0
\end{array} \right) \\
S_y & = \frac{\hbar}{2} (-i \ket{+} \bra{-} + i \ket{-} \bra{+}) \\
& = \frac{\hbar}{2} \left( \begin{array}{cc}
0 & -i \\
i & 0
\end{array} \right) \\
S_z & = \frac{\hbar}{2} (\ket{+} \bra{+} - \ket{-} \bra{-}) \\
& = \frac{\hbar}{2} \left( \begin{array}{cc} 1 & 0 \\ 0 & -1 \end{array} \right) \\
S_+ & = \hbar \ket{+} \bra{-} \\
& = \hbar \left( \begin{array}{cc}
0 & 1 \\ 0 & 0
\end{array} \right) \\
& = S_x + i S_y \\
S_- & = \hbar \ket{-} \bra{+} \\
& = \hbar \left( \begin{array}{cc}
0 & 0 \\ 1 & 0 \end{array} \right) \\
& = S_x - i S_y \\
\vec{S}^2 & = S_x^2 + S_y^2 + S_z^2 \\
& = \frac{3}{4} \hbar^2 I
\end{align*}

The eigenstates of $S_x$ are $\frac{1}{\sqrt{2}} \ket{+} + \frac{1}{\sqrt{2}} \ket{-}$ with eigenvalue $\hbar / 2$, and $\frac{1}{\sqrt{2}} \ket{+} - \frac{1}{\sqrt{2}} \ket{-}$ with eigenvalue $- \hbar / 2$.

The eigenstates of $S_y$ are $\frac{1}{\sqrt{2}} \ket{+} + \frac{i}{\sqrt{2}} \ket{-}$ with eigenvalue $\hbar/2$ and $\frac{1}{\sqrt{2}} \ket{+} - \frac{i}{\sqrt{2}} \ket{-}$ with eigenvalue $-\hbar/2$.

The eigenstates of $S_z$ are $\ket{+}$ with eigenvalue $\hbar/2$ and $\ket{-}$ with eigenvalue $-\hbar/2$.

For $a,b,c \in \{x,y,z\}$ we have
\begin{align*}
[S_a,S_b] & = i \epsilon_{abc} \hbar S_c \\
\{S_a,S_b\} & = \frac{\hbar}{2} \delta_{ab} \\
[\vec{S}^2, S_a] & = 0
\end{align*}
where $\epsilon_{abc} = 1$ if $a$, $b$, $c$ are a cyclic permutation of $x$, $y$, $z$; $\epsilon_{abc} = -1$ if $a$, $b$, $c$ are a cyclic permutation of $y$, $x$, $z$; and 0 if $a$, $b$ and $c$ are not all distinct.
\end{ex}

\begin{df}[Compatible Observables]
Observables $A$ and $B$ are \emph{compatible} iff $[A,B] = 0$; otherwise they are \emph{incompatible}.
\end{df}

\begin{thm}
\label{thm:compatible-diagonal}
Suppose that $A$ and $B$ are compatible observables, and the eigenvalues of $A$ are non-degenerate. Assume the eigenvectors of $A$ span $H$. Pick an orthonormal basis of eigenvectors of $A$. With respect to this basis, the matrix of $B$ is diagonal.
\end{thm}

\begin{proof}
\pf
\step{1}{\pflet{$\ket{u}$ and $\ket{v}$ be distinct basis elements with $A$-eigenvalues $\alpha$ and $\beta$.}}
\step{2}{$\langle u \mid [A,B] \mid v \rangle = 0$}
\step{3}{$(\alpha - \beta) \langle u \mid B \mid v \rangle = 0$}
\begin{proof}
	\pf
	\begin{align*}
		\langle u \mid [A,B] \mid v \rangle
		& = \langle u \mid AB \mid v \rangle - \langle u \mid BA \mid v \rangle \\
		& = \langle Au \mid B \mid v \rangle - \langle u \mid B \mid Av \rangle & (A \text{ is Hermitian}) \\
		& = \alpha \langle u \mid B \mid v \rangle - \beta \langle u \mid B \mid v \rangle
	\end{align*}
\end{proof}
\step{4}{$\alpha \neq \beta$}
\begin{proof}
	\pf\ Since the $\alpha$ and $\beta$ eigenspaces are non-degenerate.
\end{proof}
\step{5}{$\langle u \mid B \mid v \rangle = 0$}
\qed
\end{proof}

\begin{prop}
Let $\mathcal{H}$ be a finite dimensional Hilbert space. Let $S$ and $T$ be compatible observables.
Suppose there exists an orthonormal basis $\{ \ket{e_1}, \ldots, \ket{e_n} \}$ of eigenvectors of $T$. If $\ket{e_i}$ is a $\lambda$-eigenvector of $T$, then it is a $\langle e_i \mid S \mid e_i \rangle$-eigenvector of $S$.
\end{prop}

%TODO

\begin{cor}
Let $\mathcal{H}$ be a finite dimensional Hilbert space. Let $S$ and $T$ be compatible observables. Then every orthonormal basis of eigenvectors of $T$ is an orthonormal basis of eigenvectors of $S$.
\end{cor}


\end{document}
